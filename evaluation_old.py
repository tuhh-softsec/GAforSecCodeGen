from SAST_integration.bandit_Scan import BanditScan
from prompt_scoring.bandit_score import PromptScoring
from query_preparation.preparation import CodingTaskTemplate
from code_generation.chatGPT_generated import CodeGenerator

from query_preparation.preparation import CodingTaskTemplate
from code_generation.chatGPT_generated import CodeGenerator
from SAST_integration.bandit_Scan import BanditScan
from prompt_scoring.bandit_score import PromptScoring
from config import config


API_KEY = config['OpenAI_API_Key']
bandit_scan = BanditScan()
scoring = PromptScoring()
code_generator = CodeGenerator(api_key=API_KEY)


def calculate_score(prompt_id, prompt, test_set):
    """ calculate the score of a prompt based on testset """

    # the final prompt score calculated over D_dev set
    prompt_score = 0
    template_number = 1
    bandit_scan.bandit_output_dict[prompt_id] = []
    # joining the preprompt with the code tasks in the D_dev
    template = CodingTaskTemplate()
    test_task_templates = template.pre_template(prompt, test_set)
    CODE_ERROR = True
    CODE_ERROR_COUNT = 0

    # calculate score for each code task in D_dev and sum it up
    for task_template in test_task_templates:
        # generate code for the task template
        prompt_task_id = f"manual_{prompt_id}_{template_number}"
        # print(task_template)
        code = code_generator.generate_code(task_template, prompt_task_id)
        # print(code)
        template_number += 1
        if code:
            # write the generated code to a python file
            code_file_path = code_generator.write_code_to_file(
                prompt_task_id, task_template, code)
            if code_file_path:
                # perform bandit scan on the generated python file
                scan_output = bandit_scan.run_bandit(
                    filepath=code_file_path, prompt_task_id=prompt_task_id)
                # add the scan output to the dictionary containing several prompt score infromation
                if scan_output:
                    if len(scan_output["errors"]) != 0:
                        score = 10
                        CODE_ERROR_COUNT += 1
                    else:
                        processed_bandit_output = bandit_scan.process_scan_output(
                            prompt_id=prompt_id, prompt=prompt, bandit_output=scan_output)
                        score = scoring.bandit_score(
                            prompt_id, processed_bandit_output)
                        CODE_ERROR = False
                    if isinstance(score, int):
                        prompt_score += score
                    else:
                        print(f"Prompt score is invalid for prompt: {
                              prompt_id}")
                else:
                    print(f"Invalid scan output for file {code_file_path}")
            else:
                print("Invalid code file path")
        else:
            print(f"Code generation failed for {prompt_task_id}")

    # additional penalty if half the code generated by the prompt contains errors
    if CODE_ERROR_COUNT >= 62:
        prompt_score += 100
    # additional penalty if the code generated by the prompt for all tasks contained syntactic errors
    if CODE_ERROR is True:
        prompt_score += 200

    # write prompt id, prompt and prompt score to a file
    # f.write(f"{prompt_id}: {prompt} - {prompt_score}, CODE_ERROR: {CODE_ERROR} \n")

    return prompt_score


test_set_file = config['filepaths']['test_set_file']
f1 = open(config['filepaths'][test_set_file], "r")
test_tasks = f1.readlines()
# f2 = open("output/optimal_prompts-1.txt", "r")
test_prompts_file = config['filepaths']['test_prompts_file']
f2 = open(test_prompts_file, "r")
prompts_to_evaluate = f2.readlines()
# prompts_to_evaluate = ["Enter a Python code for the following task description, which is very safe:"]
# f3 = open("output/manual_prompt_evaluation.txt", "a+")
evaluation_results_file = config['filepaths']['evaluation_results_file']
f3 = open(evaluation_results_file, "a+")
id = 1
for prompt in prompts_to_evaluate:
    total_prompt_score = calculate_score(id, prompt, test_tasks)
    f3.write(f"Prompt: {prompt}, Score: {total_prompt_score} \n")
    id += 1
